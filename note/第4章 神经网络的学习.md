损失函数：

​	**均方误差**（mean squared error）：$E=\frac{1}{2}\sum(y_k - t_k)^2$

​	**交叉熵误差**（cross entropy error）：$E= -\sum t_k log y_k$



交叉熵：

log 表示以 e为底数的自然对数(loge)。*yk* 是神经网络的输出，*tk* 是正确解标签。并且，*tk* 中只有正确解标签的索引为 1，其他均为 0（one-hot 表示）

交叉熵误差的值是由正确解标签所对应的输出结果决定的。

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

参数 `y` 和 `t` 是 NumPy 数组。函数内部在计算 `np.log` 时，加上了一个微小值 `delta`。这是因为，当出现 `np.log(0)` 时，`np.log(0)` 会变为负无限大的 `-inf`，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。

针对于多个损失函数的公式：

$E = - \frac {1}{N} \sum_n\sum_k t_{nk}log \ y_{nk} $

备注 假设数据有 *N* 个，*tnk* 表示第 *n* 个数据的第 *k* 个元素的值（*ynk* 是神经网络的输出，*tnk* 是监督数据）。式子虽然看起来有一些复杂，其实只是把求单个数据的损失函数的式（4.2）扩大到了 *N* 份数据，不过最后还要除以 *N* 进行正规化。通过除以 *N*，可以求单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。比如，即便训练数据有 1000 个或 10000 个，也可以求得单个数据的平均损失函数

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```



**梯度**

全部变量的偏导数汇总而成的向量称为**梯度**（gradient）

```python
def numerical_gradient(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # 生成和x形状相同的数组

    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)的计算
        x[idx] = tmp_val + h
        fxh1 = f(x)

        # f(x-h)的计算
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 还原值

    return grad
```



函数的极小值、最小值以及被称为**鞍点**（saddle point）的地方，梯度为 0。极小值是局部最小值，也就是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。虽然梯度法是要寻找梯度为 0 的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。

**epoch** 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次数