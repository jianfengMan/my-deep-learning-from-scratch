SGD 的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效。因此，我们需要比单纯朝梯度方向前进的 SGD 更聪明的方法。SGD 低效的根本原因是，梯度的方向并没有指向最小值的方向。

![image-20190430150112941](../img/image-20190430150112941.png)

低效sgd的示例

![image-20190430145828303](../img/image-20190430145828303.png)



Momentum 是“动量”的意思，和物理有关。用数学式表示 Momentum 方法，如下所示。

![image-20190430150039139](../img/image-20190430150039139.png)

图 6-5 中，更新路径就像小球在碗中滚动一样。和 SGD 相比，我们发现“之”字形的“程度”减轻了。这是因为虽然 *x* 轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然 *y* 轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以 *y* 轴方向上的速度不稳定。因此，和 SGD 时的情形相比，可以更快地朝 *x* 轴方向靠近，减弱“之”字形的变动程度。

![image-20190430150213799](../img/image-20190430150213799.png)



#### AdaGrad

在神经网络的学习中，学习率（数学式中记为 *η*）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。

在关于学习率的有效技巧中，有一种被称为**学习率衰减**（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。

逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而 AdaGrad [6] 进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。

AdaGrad 会为参数的每个元素适当地调整学习率

![image-20190430150330795](../img/image-20190430150330795.png)

AdaGrad 会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 RMSProp   方法。RMSProp 方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。

![image-20190430150856858](../img/image-20190430150856858.png)



#### Adam

Momentum 参照小球在碗中滚动的物理规则进行移动，AdaGrad 为参数的每个元素适当地调整更新步伐

直观地讲，Adam就是融合了 Momentum 和 AdaGrad 的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行超参数的“偏置校正”也是 Adam 的特征

![image-20190430163208696](../img/image-20190430163208696.png)

![image-20190430163355401](../img/image-20190430163355401.png)



### 权重的初始值

在神经网络的学习中，权重的初始值特别重要。实际上，设定什么样的权重初始值，经常关系到神经网络的学习能否成功。本节将介绍权重初始值的推荐值，并通过实验确认神经网络的学习是否会快速进行。



Batch Norm，顾名思义，以进行学习时的 mini-batch 为单位，按 mini-batch 进行正规化。具体而言，就是进行使数据分布的均值为 0、方差为 1 的正规化。用数学式表示的话，如下所示。

![image-20190430171038115](../img/image-20190430171038115.png)

